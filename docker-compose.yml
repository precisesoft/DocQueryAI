services:
  # Backend service
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "5001:5001"
    volumes:
      - ./uploads:/app/uploads
      - ./documents:/app/documents
      - ./data:/app/data
      - ./tmp:/app/tmp
    environment:
      - FLASK_ENV=${FLASK_ENV:-production}
      - CHAT_MODEL=${CHAT_MODEL:-gemma2:27b}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://llm-service:11434}
    networks:
      - app-network
    depends_on:
      - llm-service
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Frontend service
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "${FRONTEND_PORT:-3000}:3000"
    depends_on:
      - backend
    networks:
      - app-network
    # Updates for production React environment configuration
    environment:
      - REACT_APP_API_URL=${REACT_APP_API_URL:-http://localhost:5001/api}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # LLM service - assuming this is a local LLM server like Ollama
  llm-service:
    image: ollama/ollama:latest
    # Do not bind to host to avoid conflicts when local Ollama is running
    # The backend accesses this service by name over the internal network.
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST:-0.0.0.0}
    networks:
      - app-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "ps"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    # Automatically pull required models on startup
    entrypoint: >
      sh -c "
        echo 'Starting Ollama service...' &&
        ollama serve &
        sleep 10 &&
        echo 'Pulling required models (embeddings + chat + vision)...' &&
        set -x && \
        ollama pull bge-m3 || echo 'bge-m3 pull failed' && \
        ollama pull gemma2:27b || echo 'gemma2:27b pull failed' && \
        # Vision models for local extraction (primary target: Gemma Vision; alt for benchmarking)
        ollama pull gemma3:12b || echo 'gemma3:12b pull failed (may not exist in registry yet)' && \
        ollama pull llama3.2-vision:11b || echo 'llama3.2-vision:11b pull failed' && \
        ollama pull llava:7b || echo 'llava:7b pull failed' && \
        set +x && \
        echo 'Model pulls attempted' &&
        wait
      "

# Shared network
networks:
  app-network:
    driver: bridge

# Volumes for persistent data
volumes:
  ollama_models:
